@article{rubin2011,
	author = {Donald B Rubin},
	title = {Causal Inference Using Potential Outcomes},
	journal = {Journal of the American Statistical Association},
	volume = {100},
	number = {469},
	pages = {322-331},
	year = {2005},
	doi = {10.1198/016214504000001880},
	URL = { 
	http://dx.doi.org/10.1198/016214504000001880
	
	},
	eprint = { 
	http://dx.doi.org/10.1198/016214504000001880
	
	}	
}




@article{austin11,
	author = { Peter C.   Austin },
	title = {An Introduction to Propensity Score Methods for Reducing the Effects of Confounding in Observational Studies},
	journal = {Multivariate Behavioral Research},
	volume = {46},
	number = {3},
	pages = {399-424},
	year = {2011},
	doi = {10.1080/00273171.2011.568786},
	note ={PMID: 21818162},
	
	URL = { 
	http://dx.doi.org/10.1080/00273171.2011.568786
	
	},
	eprint = { 
	http://dx.doi.org/10.1080/00273171.2011.568786
	
	}
	
}


@article {Abadie2016,
	author = {Abadie, Alberto and Imbens, Guido W.},
	title = {Matching on the Estimated Propensity Score},
	journal = {Econometrica},
	volume = {84},
	number = {2},
	publisher = {Blackwell Publishing Ltd},
	issn = {1468-0262},
	url = {http://dx.doi.org/10.3982/ECTA11293},
	doi = {10.3982/ECTA11293},
	pages = {781--807},
	keywords = {
	Matching estimators
	, 
	propensity score matching
	, 
	average treatment effects
	, 
	causal inference
	, 
	program evaluation
	},
	year = {2016},
}

@article{rosenbaum83,
	author = {Rosenbaum, PAUL R. and Rubin, DONALD B.},
	title = {The central role of the propensity score in observational studies for causal effects},
	journal = {Biometrika},
	volume = {70},
	number = {1},
	pages = {41-55},
	year = {1983},
	doi = {10.1093/biomet/70.1.41},
	URL = { + http://dx.doi.org/10.1093/biomet/70.1.41},
	eprint = {/oup/backfile/content_public/journal/biomet/70/1/10.1093/biomet/70.1.41/2/70-1-41.pdf}
}


@article{rubin73,
	ISSN = {0006341X, 15410420},
	URL = {http://www.jstor.org/stable/2529684},
	abstract = {Several matching methods that match all of one sample from another larger sample on a continuous matching variable are compared with respect to their ability to remove the bias of the matching variable. One method is a simple mean-matching method and three are nearest available pair-matching methods. The methods' abilities to remove bias are also compared with the theoretical maximum given fixed distributions and fixed sample sizes. A summary of advice to an investigator is included.},
	author = {Donald B. Rubin},
	journal = {Biometrics},
	number = {1},
	pages = {159-183},
	publisher = {[Wiley, International Biometric Society]},
	title = {Matching to Remove Bias in Observational Studies},
	volume = {29},
	year = {1973}
}

@inbook{sontag1,
	title = "Learning representations for counterfactual inference",
	abstract = "Observational studies are rising in importance due to the widespread accumulation of data in fields such as healthcare, education, employment and ecology. We consider the task of answering counterfactual questions such as, {"}Would this patient have lower blood sugar had she received a different medication?{"}. We propose a new algorithmic framework for counterfactual inference which brings together ideas from domain adaptation and representation learning. In addition to a theoretical justification, we perform an empirical comparison with previous approaches to causal inference from observational data. Our deep learning algorithm significantly outperforms the previous state-of-the-art.",
	author = "Johansson, {Fredrik D.} and Uri Shalit and David Sontag",
	year = "2016",
	volume = "6",
	pages = "4407--4418",
	booktitle = "33rd International Conference on Machine Learning, ICML 2016",
	publisher = "International Machine Learning Society (IMLS)",
}


@InProceedings{sontag2,
	title = 	 {Estimating individual treatment effect: generalization bounds and algorithms},
	author = 	 {Uri Shalit and Fredrik D. Johansson and David Sontag},
	booktitle = 	 {Proceedings of the 34th International Conference on Machine Learning},
	pages = 	 {3076--3085},
	year = 	 {2017},
	editor = 	 {Doina Precup and Yee Whye Teh},
	volume = 	 {70},
	series = 	 {Proceedings of Machine Learning Research},
	address = 	 {International Convention Centre, Sydney, Australia},
	month = 	 {06--11 Aug},
	publisher = 	 {PMLR},
	pdf = 	 {http://proceedings.mlr.press/v70/shalit17a/shalit17a.pdf},
	url = 	 {http://proceedings.mlr.press/v70/shalit17a.html},
	abstract = 	 {There is intense interest in applying machine learning to problems of causal inference in fields such as healthcare, economics and education. In particular, individual-level causal inference has important applications such as precision medicine. We give a new theoretical analysis and family of algorithms for predicting individual treatment effect (ITE) from observational data, under the assumption known as strong ignorability. The algorithms learn a “balanced” representation such that the induced treated and control distributions look similar, and we give a novel and intuitive generalization-error bound showing the expected ITE estimation error of a representation is bounded by a sum of the standard generalization-error of that representation and the distance between the treated and control distributions induced by the representation. We use Integral Probability Metrics to measure distances between distributions, deriving explicit bounds for the Wasserstein and Maximum Mean Discrepancy (MMD) distances. Experiments on real and simulated data show the new algorithms match or outperform the state-of-the-art.}
}




@article{hill,
	author = {Jennifer L. Hill},
	title = {Bayesian Nonparametric Modeling for Causal Inference},
	journal = {Journal of Computational and Graphical Statistics},
	volume = {20},
	number = {1},
	pages = {217-240},
	year = {2011},
	doi = {10.1198/jcgs.2010.08162},
	
	URL = { 
	http://dx.doi.org/10.1198/jcgs.2010.08162
	
	},
	eprint = { 
	http://dx.doi.org/10.1198/jcgs.2010.08162
	
	}
	
}

@ARTICLE{wager,
	author = {{Wager}, S. and {Athey}, S.},
	title = "{Estimation and Inference of Heterogeneous Treatment Effects using Random Forests}",
	journal = {ArXiv e-prints},
	archivePrefix = "arXiv",
	eprint = {1510.04342},
	primaryClass = "stat.ME",
	keywords = {Statistics - Methodology, Mathematics - Statistics Theory, Statistics - Machine Learning},
	year = 2015,
	month = oct,
	adsurl = {http://adsabs.harvard.edu/abs/2015arXiv151004342W},
	adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{propensity-dropout,
	author = {{Alaa}, A.~M. and {Weisz}, M. and {van der Schaar}, M.},
	title = "{Deep Counterfactual Networks with Propensity-Dropout}",
	journal = {ArXiv e-prints},
	archivePrefix = "arXiv",
	eprint = {1706.05966},
	primaryClass = "cs.LG",
	keywords = {Computer Science - Learning, Statistics - Machine Learning},
	year = 2017,
	month = jun,
	adsurl = {http://adsabs.harvard.edu/abs/2017arXiv170605966A},
	adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}





